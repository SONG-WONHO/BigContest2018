{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 임포트\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset 로딩\n",
    "x_load = pd.read_csv('C:\\\\Users\\\\zeus_\\\\Desktop\\\\champion_data\\\\train\\\\train_data.csv').sort_values(by='acc_id', ascending=True)\n",
    "y_load = pd.read_csv('C:\\\\Users\\\\zeus_\\\\Desktop\\\\champion_data\\\\train\\\\train_label.csv').sort_values(by='acc_id', ascending=True)['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = x_load\n",
    "y_data = y_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = x_data.drop(['acc_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_filter = [v[0] or v[1] for v in zip(list((y_data == 'month').values), list((y_data == '2month').values))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_data = x_data[custom_filter]\n",
    "# y_data = y_data[custom_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.1, random_state=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train['play_time_8_square'] = x_train['play_time_8'] ** 2\n",
    "# x_train['play_time_8_cube'] = x_train['play_time_8'] ** 3\n",
    "# for i in range(1,8):\n",
    "#     temp_a = 'play_time_' + str(i) + '_to_8'\n",
    "#     temp_b = 'play_time_' + str(i)\n",
    "#     x_train[temp_a] = ((x_train['play_time_8'] - x_train['play_time_8'].min()) / (x_train['play_time_8'].max() - x_train['play_time_8'].min()))/((x_train[temp_b] - x_train[temp_b].min()) / (x_train[temp_b].max() - x_train[temp_b].min())   + 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test['play_time_8_square'] = x_test['play_time_8'] ** 3\n",
    "# x_test['play_time_8_cube'] = x_test['play_time_8'] ** 3\n",
    "# for i in range(1,8):\n",
    "#     temp_a = 'play_time_' + str(i) + '_to_8'\n",
    "#     temp_b = 'play_time_' + str(i)\n",
    "#     x_test[temp_a] = ((x_test['play_time_8'] - x_test['play_time_8'].min()) / (x_test['play_time_8'].max() - x_test['play_time_8'].min()))/((x_test[temp_b] - x_test[temp_b].min()) / (x_test[temp_b].max() - x_test[temp_b].min())   + 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_filter = ['party_cnt_8', 'party_cnt', 'party_chat_8', 'play_time_7_to_8', 'play_time_6_to_8','play_time_5_to_8','play_time_4_to_8','play_time_3_to_8','play_time_2_to_8','play_time_1_to_8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = []\n",
    "\n",
    "# for v in range(1,9):\n",
    "#     temp.append('play_time_' + str(v))\n",
    "#     temp.append('cnt_dt_' + str(v))\n",
    "#     temp.append('item_hongmun_' + str(v))\n",
    "#     temp.append('game_combat_time_' + str(v))\n",
    "    \n",
    "# print(temp)\n",
    "# temp.extend(add_filter)\n",
    "# print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = x_train[temp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Machine\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S.V.M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svm = LinearSVC().fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 세트 정확도: 0.340\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 세트 정확도: {:.3f}\".format(linear_svm.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "     2month       0.29      0.51      0.37      2551\n",
      "      month       0.34      0.45      0.39      2466\n",
      "   retained       0.34      0.04      0.08      2454\n",
      "       week       0.46      0.35      0.40      2529\n",
      "\n",
      "avg / total       0.36      0.34      0.31     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, linear_svm.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 커널 S.V.M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel='rbf', C=10, gamma=0.01, class_weight={'month':1.4, '2month':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, cache_size=200, class_weight={'month': 1.4, '2month': 1}, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.01, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 세트 정확도: 0.666\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 세트 정확도: {:.3f}\".format(svm.score(x_test[temp], y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "     2month       0.73      0.53      0.62      2510\n",
      "      month       0.63      0.80      0.70      2490\n",
      "\n",
      "avg / total       0.68      0.67      0.66      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, svm.predict(x_test[temp])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 의사결정나무"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(criterion='entropy', max_depth=30, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=30,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 세트 정확도: 0.679\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 세트 정확도: {:.3f}\".format(tree.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "     2month       0.59      0.59      0.59      2551\n",
      "      month       0.56      0.55      0.56      2466\n",
      "   retained       0.74      0.74      0.74      2454\n",
      "       week       0.83      0.83      0.83      2529\n",
      "\n",
      "avg / total       0.68      0.68      0.68     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, tree.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그래디언트 부스팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingClassifier(random_state=1, max_depth=1, learning_rate=0.1, n_estimators=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=1,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=80,\n",
       "              presort='auto', random_state=1, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 세트 정확도: 0.635\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 세트 정확도: {:.3f}\".format(gbrt.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "     2month       0.51      0.77      0.61      2551\n",
      "      month       0.61      0.27      0.37      2466\n",
      "   retained       0.71      0.71      0.71      2454\n",
      "       week       0.77      0.78      0.78      2529\n",
      "\n",
      "avg / total       0.65      0.63      0.62     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, gbrt.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cv = x_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cv = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#한국어로 된 레이블을 숫자로 인코딩\n",
    "idx2label = list(set(y_train.values))\n",
    "label2idx = {c:i for i,c in enumerate(idx2label)}\n",
    "\n",
    "#new y_train & y_cv\n",
    "y_train_new = [label2idx[v] for v in y_train]\n",
    "y_cv_new = [label2idx[v] for v in y_cv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new = x_train.values\n",
    "x_cv_new = x_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이블 개수\n",
    "num_feature = len(x_train.values[0])\n",
    "learning_rate = 1e-4\n",
    "training_epoch = 100\n",
    "batch_size = 100\n",
    "label = len(idx2label)\n",
    "gamma = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, num_feature])\n",
    "Y = tf.placeholder(tf.int32, [None])\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y_one_hot\n",
    "Y_one_hot = tf.one_hot(Y, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layer1\n",
    "with tf.name_scope('layer1') as scope:\n",
    "    w1 = tf.get_variable(\"weight1\", [num_feature, 128], tf.float32, layers.xavier_initializer())\n",
    "    b1 = tf.Variable(tf.random_normal([128]), name = \"bias1\")\n",
    "        \n",
    "    layer1 = tf.nn.relu(tf.matmul(X,w1) + b1)\n",
    "    layer1 = tf.nn.dropout(layer1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layer2\n",
    "with tf.name_scope('layer2') as scope:\n",
    "    w2 = tf.get_variable(\"weight2\", [128, 64], tf.float32, layers.xavier_initializer())\n",
    "    b2 = tf.Variable(tf.random_normal([64]), name = \"bias2\")\n",
    "    \n",
    "    layer2 = tf.nn.relu(tf.matmul(layer1,w2) + b2)\n",
    "    layer2 = tf.nn.dropout(layer2, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layer3\n",
    "with tf.name_scope('layer3') as scope:\n",
    "    w3 = tf.get_variable(\"weight3\", [64, 32], tf.float32, layers.xavier_initializer())\n",
    "    b3 = tf.Variable(tf.random_normal([32]), name = \"bias3\")\n",
    "    \n",
    "    layer3 = tf.nn.relu(tf.matmul(layer2,w3) + b3)\n",
    "    layer3 = tf.nn.dropout(layer3, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layer4\n",
    "with tf.name_scope('layer4') as scope:\n",
    "    w4 = tf.get_variable(\"weight4\", [32, 32], tf.float32, layers.xavier_initializer())\n",
    "    b4 = tf.Variable(tf.random_normal([32]), name = \"bias4\")\n",
    "    \n",
    "    layer4 = tf.nn.relu(tf.matmul(layer3,w4) + b4)\n",
    "    layer4 = tf.nn.dropout(layer4, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layer5\n",
    "with tf.name_scope('layer5') as scope:\n",
    "    w5 = tf.get_variable(\"weight5\", [32, 16], tf.float32, layers.xavier_initializer())\n",
    "    b5 = tf.Variable(tf.random_normal([16]), name = \"bias5\")\n",
    "    \n",
    "    layer5 = tf.nn.relu(tf.matmul(layer4,w5) + b5)\n",
    "    layer5 = tf.nn.dropout(layer5, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layer6\n",
    "with tf.name_scope('layer6') as scope:\n",
    "    w6 = tf.get_variable(\"weight6\", [16, 8], tf.float32, layers.xavier_initializer())\n",
    "    b6 = tf.Variable(tf.random_normal([8]), name = \"bias6\")\n",
    "    \n",
    "    layer6 = tf.nn.relu(tf.matmul(layer5,w6) + b6)\n",
    "    layer6 = tf.nn.dropout(layer6, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hypothesis\n",
    "with tf.name_scope('hypothesis') as scope:\n",
    "    wh = tf.get_variable(\"weighth\", [8, label], tf.float32, layers.xavier_initializer())\n",
    "    bh = tf.Variable(tf.random_normal([label]), name = \"biash\")\n",
    "    \n",
    "    logits = tf.matmul(layer6, wh) + bh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cost Function - softmax cross entropy\n",
    "with tf.name_scope(\"cost\") as scope:\n",
    "    cost = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y_one_hot)\n",
    "    cost = tf.reduce_mean(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train - Gradient Descent\n",
    "with tf.name_scope(\"train\") as scope:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction\n",
    "prediction = tf.argmax(logits, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy\n",
    "with tf.name_scope(\"accuracy\") as scope:\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, tf.argmax(Y_one_hot,-1)), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch : 0]  cost 236055252.65888903\n",
      "[epoch : 1]  cost 114441925.9666666\n",
      "[epoch : 2]  cost 60761661.44250003\n",
      "[epoch : 3]  cost 47886378.7015278\n",
      "[epoch : 4]  cost 35655296.6459028\n",
      "[epoch : 5]  cost 26747123.850833368\n",
      "[epoch : 6]  cost 21791001.645486113\n",
      "[epoch : 7]  cost 14709288.199670129\n",
      "[epoch : 8]  cost 11412983.676666673\n",
      "[epoch : 9]  cost 8832148.953585066\n",
      "[epoch : 10]  cost 9443148.539114598\n",
      "[epoch : 11]  cost 6152777.416178394\n",
      "[epoch : 12]  cost 4926444.921243491\n",
      "[epoch : 13]  cost 4807800.573559027\n",
      "[epoch : 14]  cost 2799409.2953342055\n",
      "[epoch : 15]  cost 3144372.048207464\n",
      "[epoch : 16]  cost 2363864.712881403\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-fded97fa57cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mavg_cost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mtotal_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\venv\\data_science\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mvalues\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   4631\u001b[0m         \"\"\"\n\u001b[0;32m   4632\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4633\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_AXIS_REVERSED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4635\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\venv\\data_science\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mas_array\u001b[1;34m(self, transpose, items)\u001b[0m\n\u001b[0;32m   3947\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3948\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3949\u001b[1;33m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interleave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3950\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3951\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtranspose\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\venv\\data_science\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m_interleave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3976\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3977\u001b[0m             \u001b[0mrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3978\u001b[1;33m             \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3979\u001b[0m             \u001b[0mitemmask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(training_epoch):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(len(x_train.values) / batch_size)\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_x = x_train_new[i*batch_size:(i+1)*batch_size]\n",
    "            batch_y = y_train_new[i*batch_size:(i+1)*batch_size]\n",
    "            \n",
    "            for i in range(1):\n",
    "                cost_val, _ = sess.run([cost, train], feed_dict={X:batch_x,Y:batch_y, keep_prob:0.7})\n",
    "            avg_cost += cost_val/total_batch\n",
    "        \n",
    "        print(\"[epoch : {}]  cost {}\".format(epoch, avg_cost))\n",
    "    \n",
    "    print(\"accuracy : {}\".format(sess.run(accuracy, feed_dict={X:x_train_new, Y:y_train_new, keep_prob:1})))\n",
    "    print(\"accuracy : {}\".format(sess.run(accuracy, feed_dict={X:x_cv_new, Y:y_cv_new, keep_prob:1})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(columns=[\"new1\",\"new2\"], data=result).to_csv('C:\\\\Users\\\\zeus_\\\\Desktop\\\\champion_data\\\\train\\\\10_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report([idx2label[v] for v in y_cv], [idx2label[v] for v in result]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
