{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine <- read.csv('C:\\\\Users\\\\zeus_\\\\Desktop\\\\data_1\\\\data4_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "set.seed(10)\n",
    "train_test_split <- sample(1:nrow(wine), 0.8 * nrow(wine))\n",
    "\n",
    "train_set <- wine[train_test_split, ]\n",
    "test_set <- wine[-train_test_split, ]\n",
    "\n",
    "write.csv(test_set, 'C:\\\\Users\\\\zeus_\\\\Desktop\\\\data_1\\\\test_set.csv', row.names = F)\n",
    "write.csv(train_set, 'C:\\\\Users\\\\zeus_\\\\Desktop\\\\data_1\\\\train_set.csv', row.names = F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>used</th><th scope=col>(Mb)</th><th scope=col>gc trigger</th><th scope=col>(Mb)</th><th scope=col>max used</th><th scope=col>(Mb)</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Ncells</th><td>1603379</td><td>85.7   </td><td>3223614</td><td>172.2  </td><td>3223614</td><td>172.2  </td></tr>\n",
       "\t<tr><th scope=row>Vcells</th><td>2927945</td><td>22.4   </td><td>8388608</td><td> 64.0  </td><td>8383032</td><td> 64.0  </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllll}\n",
       "  & used & (Mb) & gc trigger & (Mb) & max used & (Mb)\\\\\n",
       "\\hline\n",
       "\tNcells & 1603379 & 85.7    & 3223614 & 172.2   & 3223614 & 172.2  \\\\\n",
       "\tVcells & 2927945 & 22.4    & 8388608 &  64.0   & 8383032 &  64.0  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | used | (Mb) | gc trigger | (Mb) | max used | (Mb) | \n",
       "|---|---|\n",
       "| Ncells | 1603379 | 85.7    | 3223614 | 172.2   | 3223614 | 172.2   | \n",
       "| Vcells | 2927945 | 22.4    | 8388608 |  64.0   | 8383032 |  64.0   | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "       used    (Mb) gc trigger (Mb)  max used (Mb) \n",
       "Ncells 1603379 85.7 3223614    172.2 3223614  172.2\n",
       "Vcells 2927945 22.4 8388608     64.0 8383032   64.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rm(test_set)\n",
    "rm(wine)\n",
    "gc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(10)\n",
    "train_validation_split <- sample(1:nrow(train_set), 0.8 * nrow(train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이진회귀모델\n",
    "Model_LR <- glm(type~. ,data = train_set[train_validation_split, ], family='binomial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred <- predict(Model_LR, train_set[-train_validation_split,], type='response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label <- (pred > 0.5) * 1\n",
    "real_label <- train_set$type[-train_validation_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          pred_label\n",
       "real_label   0   1\n",
       "         0 251   6\n",
       "         1   3 780"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table(real_label, pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.00865384615384615"
      ],
      "text/latex": [
       "0.00865384615384615"
      ],
      "text/markdown": [
       "0.00865384615384615"
      ],
      "text/plain": [
       "[1] 0.008653846"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean((real_label - pred_label)^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#부스팅\n",
    "library(xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "input <- model.matrix(quality~.,data=train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "label <- train_set$quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "하이퍼파라미터 <- list(eta=0.3, gamma = 0, max_depth=6, subsample=0.9, colsample_bytree=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttrain-rmse:3.817774+0.002821\ttest-rmse:3.819980+0.014487 \n",
      "Multiple eval metrics are present. Will use test_rmse for early stopping.\n",
      "Will train until test_rmse hasn't improved in 20 rounds.\n",
      "\n",
      "[2]\ttrain-rmse:2.723959+0.002443\ttest-rmse:2.729091+0.015517 \n",
      "[3]\ttrain-rmse:1.972266+0.002458\ttest-rmse:1.982713+0.015114 \n",
      "[4]\ttrain-rmse:1.461438+0.003152\ttest-rmse:1.480740+0.014346 \n",
      "[5]\ttrain-rmse:1.119530+0.003493\ttest-rmse:1.153358+0.014416 \n",
      "[6]\ttrain-rmse:0.899528+0.004831\ttest-rmse:0.948840+0.013020 \n",
      "[7]\ttrain-rmse:0.762799+0.005870\ttest-rmse:0.826434+0.013278 \n",
      "[8]\ttrain-rmse:0.679696+0.007553\ttest-rmse:0.759016+0.013882 \n",
      "[9]\ttrain-rmse:0.630072+0.007969\ttest-rmse:0.722399+0.015227 \n",
      "[10]\ttrain-rmse:0.599656+0.006982\ttest-rmse:0.701914+0.015017 \n",
      "[11]\ttrain-rmse:0.579486+0.006898\ttest-rmse:0.691386+0.015458 \n",
      "[12]\ttrain-rmse:0.563391+0.006025\ttest-rmse:0.684393+0.015447 \n",
      "[13]\ttrain-rmse:0.551423+0.005177\ttest-rmse:0.680121+0.014496 \n",
      "[14]\ttrain-rmse:0.540944+0.006314\ttest-rmse:0.677187+0.014179 \n",
      "[15]\ttrain-rmse:0.532194+0.006326\ttest-rmse:0.673451+0.013352 \n",
      "[16]\ttrain-rmse:0.528648+0.007371\ttest-rmse:0.672500+0.013095 \n",
      "[17]\ttrain-rmse:0.520114+0.006152\ttest-rmse:0.670284+0.012731 \n",
      "[18]\ttrain-rmse:0.515288+0.004561\ttest-rmse:0.669156+0.012867 \n",
      "[19]\ttrain-rmse:0.509490+0.004077\ttest-rmse:0.668008+0.013260 \n",
      "[20]\ttrain-rmse:0.504668+0.004574\ttest-rmse:0.668289+0.013526 \n",
      "[21]\ttrain-rmse:0.499435+0.005487\ttest-rmse:0.667871+0.012600 \n",
      "[22]\ttrain-rmse:0.494478+0.004014\ttest-rmse:0.668148+0.012917 \n",
      "[23]\ttrain-rmse:0.488204+0.003884\ttest-rmse:0.667341+0.012627 \n",
      "[24]\ttrain-rmse:0.483323+0.004309\ttest-rmse:0.666085+0.012272 \n",
      "[25]\ttrain-rmse:0.477505+0.004757\ttest-rmse:0.665139+0.011844 \n",
      "[26]\ttrain-rmse:0.472384+0.005411\ttest-rmse:0.664551+0.012582 \n",
      "[27]\ttrain-rmse:0.467519+0.004540\ttest-rmse:0.664200+0.013184 \n",
      "[28]\ttrain-rmse:0.460741+0.003827\ttest-rmse:0.663602+0.012261 \n",
      "[29]\ttrain-rmse:0.455262+0.002122\ttest-rmse:0.663494+0.012582 \n",
      "[30]\ttrain-rmse:0.449675+0.002744\ttest-rmse:0.663293+0.012721 \n",
      "[31]\ttrain-rmse:0.443153+0.003036\ttest-rmse:0.663082+0.012968 \n",
      "[32]\ttrain-rmse:0.437821+0.002399\ttest-rmse:0.663583+0.012589 \n",
      "[33]\ttrain-rmse:0.433880+0.002732\ttest-rmse:0.663088+0.012838 \n",
      "[34]\ttrain-rmse:0.427091+0.003997\ttest-rmse:0.662839+0.012000 \n",
      "[35]\ttrain-rmse:0.422481+0.004197\ttest-rmse:0.662862+0.011737 \n",
      "[36]\ttrain-rmse:0.416860+0.005944\ttest-rmse:0.661458+0.011665 \n",
      "[37]\ttrain-rmse:0.413603+0.004755\ttest-rmse:0.661031+0.011680 \n",
      "[38]\ttrain-rmse:0.410773+0.005594\ttest-rmse:0.661148+0.012182 \n",
      "[39]\ttrain-rmse:0.406337+0.005604\ttest-rmse:0.660996+0.011889 \n",
      "[40]\ttrain-rmse:0.402007+0.005079\ttest-rmse:0.661147+0.012517 \n",
      "[41]\ttrain-rmse:0.397919+0.004968\ttest-rmse:0.661416+0.011916 \n",
      "[42]\ttrain-rmse:0.393357+0.004444\ttest-rmse:0.661122+0.011898 \n",
      "[43]\ttrain-rmse:0.390002+0.005423\ttest-rmse:0.660922+0.011837 \n",
      "[44]\ttrain-rmse:0.385762+0.005853\ttest-rmse:0.660378+0.011627 \n",
      "[45]\ttrain-rmse:0.380883+0.006779\ttest-rmse:0.660913+0.011445 \n",
      "[46]\ttrain-rmse:0.376530+0.006112\ttest-rmse:0.660875+0.011532 \n",
      "[47]\ttrain-rmse:0.370943+0.005675\ttest-rmse:0.660399+0.011822 \n",
      "[48]\ttrain-rmse:0.367814+0.005652\ttest-rmse:0.659671+0.012080 \n",
      "[49]\ttrain-rmse:0.364623+0.006872\ttest-rmse:0.659460+0.011997 \n",
      "[50]\ttrain-rmse:0.361237+0.007793\ttest-rmse:0.659466+0.012047 \n",
      "[51]\ttrain-rmse:0.357894+0.007372\ttest-rmse:0.659407+0.012183 \n",
      "[52]\ttrain-rmse:0.354464+0.007724\ttest-rmse:0.659360+0.012183 \n",
      "[53]\ttrain-rmse:0.351137+0.006859\ttest-rmse:0.658857+0.012526 \n",
      "[54]\ttrain-rmse:0.347732+0.007658\ttest-rmse:0.658215+0.013381 \n",
      "[55]\ttrain-rmse:0.344585+0.007185\ttest-rmse:0.658189+0.013238 \n",
      "[56]\ttrain-rmse:0.341259+0.007408\ttest-rmse:0.658093+0.013271 \n",
      "[57]\ttrain-rmse:0.336644+0.007876\ttest-rmse:0.657742+0.012968 \n",
      "[58]\ttrain-rmse:0.333403+0.008359\ttest-rmse:0.657260+0.013192 \n",
      "[59]\ttrain-rmse:0.330046+0.008305\ttest-rmse:0.657229+0.012491 \n",
      "[60]\ttrain-rmse:0.326918+0.007633\ttest-rmse:0.656865+0.012556 \n",
      "[61]\ttrain-rmse:0.324270+0.007298\ttest-rmse:0.657207+0.012071 \n",
      "[62]\ttrain-rmse:0.320947+0.007375\ttest-rmse:0.657117+0.011906 \n",
      "[63]\ttrain-rmse:0.316994+0.007094\ttest-rmse:0.657283+0.012139 \n",
      "[64]\ttrain-rmse:0.313554+0.007917\ttest-rmse:0.657248+0.012585 \n",
      "[65]\ttrain-rmse:0.311262+0.007195\ttest-rmse:0.657582+0.012584 \n",
      "[66]\ttrain-rmse:0.307596+0.006469\ttest-rmse:0.657572+0.012745 \n",
      "[67]\ttrain-rmse:0.304350+0.006332\ttest-rmse:0.657752+0.012703 \n",
      "[68]\ttrain-rmse:0.301360+0.004889\ttest-rmse:0.657461+0.012973 \n",
      "[69]\ttrain-rmse:0.299282+0.005089\ttest-rmse:0.657344+0.013180 \n",
      "[70]\ttrain-rmse:0.296978+0.005193\ttest-rmse:0.657223+0.012961 \n",
      "[71]\ttrain-rmse:0.294465+0.005487\ttest-rmse:0.656881+0.013047 \n",
      "[72]\ttrain-rmse:0.291277+0.005647\ttest-rmse:0.657041+0.012751 \n",
      "[73]\ttrain-rmse:0.288351+0.006184\ttest-rmse:0.656302+0.012537 \n",
      "[74]\ttrain-rmse:0.285424+0.006160\ttest-rmse:0.656307+0.012019 \n",
      "[75]\ttrain-rmse:0.283608+0.007307\ttest-rmse:0.656023+0.011737 \n",
      "[76]\ttrain-rmse:0.281225+0.006710\ttest-rmse:0.655896+0.012485 \n",
      "[77]\ttrain-rmse:0.278248+0.007248\ttest-rmse:0.655992+0.012079 \n",
      "[78]\ttrain-rmse:0.275624+0.007618\ttest-rmse:0.655792+0.012354 \n",
      "[79]\ttrain-rmse:0.273367+0.008143\ttest-rmse:0.655726+0.012203 \n",
      "[80]\ttrain-rmse:0.270756+0.007819\ttest-rmse:0.655438+0.012817 \n",
      "[81]\ttrain-rmse:0.268193+0.008259\ttest-rmse:0.655219+0.012774 \n",
      "[82]\ttrain-rmse:0.265644+0.007736\ttest-rmse:0.654938+0.012939 \n",
      "[83]\ttrain-rmse:0.262484+0.006701\ttest-rmse:0.654671+0.013392 \n",
      "[84]\ttrain-rmse:0.259595+0.006856\ttest-rmse:0.655058+0.013555 \n",
      "[85]\ttrain-rmse:0.256982+0.006086\ttest-rmse:0.655001+0.013930 \n",
      "[86]\ttrain-rmse:0.254708+0.006016\ttest-rmse:0.655319+0.013984 \n",
      "[87]\ttrain-rmse:0.252452+0.005885\ttest-rmse:0.655072+0.013900 \n",
      "[88]\ttrain-rmse:0.250120+0.005598\ttest-rmse:0.654992+0.013956 \n",
      "[89]\ttrain-rmse:0.247718+0.005928\ttest-rmse:0.655087+0.013530 \n",
      "[90]\ttrain-rmse:0.244970+0.005626\ttest-rmse:0.654929+0.013311 \n",
      "[91]\ttrain-rmse:0.242686+0.005391\ttest-rmse:0.654717+0.013248 \n",
      "[92]\ttrain-rmse:0.239932+0.004832\ttest-rmse:0.654617+0.013471 \n",
      "[93]\ttrain-rmse:0.238018+0.005174\ttest-rmse:0.654504+0.013453 \n",
      "[94]\ttrain-rmse:0.235681+0.005573\ttest-rmse:0.654376+0.013349 \n",
      "[95]\ttrain-rmse:0.232879+0.005489\ttest-rmse:0.654557+0.013096 \n",
      "[96]\ttrain-rmse:0.231028+0.005316\ttest-rmse:0.654282+0.013260 \n",
      "[97]\ttrain-rmse:0.228971+0.005325\ttest-rmse:0.654147+0.013092 \n",
      "[98]\ttrain-rmse:0.226326+0.006034\ttest-rmse:0.654356+0.013177 \n",
      "[99]\ttrain-rmse:0.224657+0.005836\ttest-rmse:0.654166+0.013356 \n",
      "[100]\ttrain-rmse:0.222401+0.005270\ttest-rmse:0.654506+0.013632 \n",
      "[101]\ttrain-rmse:0.220568+0.005029\ttest-rmse:0.654555+0.013854 \n",
      "[102]\ttrain-rmse:0.218644+0.004740\ttest-rmse:0.654406+0.013772 \n",
      "[103]\ttrain-rmse:0.215987+0.004255\ttest-rmse:0.653963+0.014098 \n",
      "[104]\ttrain-rmse:0.215073+0.004380\ttest-rmse:0.653990+0.014083 \n",
      "[105]\ttrain-rmse:0.212817+0.004448\ttest-rmse:0.653507+0.014082 \n",
      "[106]\ttrain-rmse:0.210746+0.003955\ttest-rmse:0.653746+0.013836 \n",
      "[107]\ttrain-rmse:0.208412+0.003519\ttest-rmse:0.653607+0.014066 \n",
      "[108]\ttrain-rmse:0.206293+0.003248\ttest-rmse:0.653490+0.013783 \n",
      "[109]\ttrain-rmse:0.204154+0.002747\ttest-rmse:0.653779+0.013862 \n",
      "[110]\ttrain-rmse:0.201890+0.002933\ttest-rmse:0.653734+0.013759 \n",
      "[111]\ttrain-rmse:0.199843+0.003085\ttest-rmse:0.653668+0.013840 \n",
      "[112]\ttrain-rmse:0.197945+0.003505\ttest-rmse:0.653374+0.013753 \n",
      "[113]\ttrain-rmse:0.195460+0.003300\ttest-rmse:0.653163+0.013557 \n",
      "[114]\ttrain-rmse:0.193180+0.003737\ttest-rmse:0.653507+0.013556 \n",
      "[115]\ttrain-rmse:0.191497+0.003717\ttest-rmse:0.653442+0.013415 \n",
      "[116]\ttrain-rmse:0.189687+0.003900\ttest-rmse:0.653261+0.013417 \n",
      "[117]\ttrain-rmse:0.188018+0.003460\ttest-rmse:0.653276+0.013549 \n",
      "[118]\ttrain-rmse:0.186202+0.004009\ttest-rmse:0.653257+0.013095 \n",
      "[119]\ttrain-rmse:0.184489+0.004113\ttest-rmse:0.653255+0.013141 \n",
      "[120]\ttrain-rmse:0.182643+0.004167\ttest-rmse:0.653167+0.013078 \n",
      "[121]\ttrain-rmse:0.180902+0.003862\ttest-rmse:0.653409+0.012948 \n",
      "[122]\ttrain-rmse:0.178837+0.003289\ttest-rmse:0.653691+0.012666 \n",
      "[123]\ttrain-rmse:0.177229+0.003188\ttest-rmse:0.653845+0.012405 \n",
      "[124]\ttrain-rmse:0.175742+0.003216\ttest-rmse:0.653712+0.012294 \n",
      "[125]\ttrain-rmse:0.174306+0.003119\ttest-rmse:0.653712+0.012261 \n",
      "[126]\ttrain-rmse:0.172615+0.002762\ttest-rmse:0.654327+0.012168 \n",
      "[127]\ttrain-rmse:0.170938+0.002893\ttest-rmse:0.654366+0.012278 \n",
      "[128]\ttrain-rmse:0.169721+0.003074\ttest-rmse:0.654577+0.012288 \n",
      "[129]\ttrain-rmse:0.168446+0.003428\ttest-rmse:0.654477+0.012010 \n",
      "[130]\ttrain-rmse:0.167161+0.003304\ttest-rmse:0.654609+0.011991 \n",
      "[131]\ttrain-rmse:0.165658+0.003091\ttest-rmse:0.654626+0.012229 \n",
      "[132]\ttrain-rmse:0.164741+0.003496\ttest-rmse:0.654575+0.012157 \n",
      "[133]\ttrain-rmse:0.163507+0.003764\ttest-rmse:0.654463+0.012172 \n",
      "Stopping. Best iteration:\n",
      "[113]\ttrain-rmse:0.195460+0.003300\ttest-rmse:0.653163+0.013557\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Model <- xgb.cv(data = input, label = label, params = 하이퍼파라미터, nfold=5, nrounds = 999999, early_stopping_rounds = 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
